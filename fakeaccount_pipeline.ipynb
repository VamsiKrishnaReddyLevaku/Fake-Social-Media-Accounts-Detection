{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score, roc_curve)\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "OUTDIR = './outputs'\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR,'plots'), exist_ok=True)\n",
    "print('Output dir:', OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_excel(\"fake_dataset.xlsx\")\n",
    "df.to_csv(\"fake_dataset.csv\", index=False)\n",
    "\n",
    "\n",
    "print('Loaded', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1) Shape:', df.shape)\n",
    "print('2) Columns & dtypes:')\n",
    "display(df.dtypes.to_frame('dtype').join(df.count().to_frame('non_null')))\n",
    "print('\\n3) Missing value percentage (top):')\n",
    "miss_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "display(miss_pct[miss_pct>0].head(30))\n",
    "print('\\n4) Basic stats for numeric columns:')\n",
    "display(df.describe().T)\n",
    "print('\\n5) First 5 rows:')\n",
    "display(df.head())\n",
    "with open(os.path.join(OUTDIR,'inspection.txt'),'w') as f:\n",
    "    f.write('Shape: %s\\n\\n' % (str(df.shape)))\n",
    "    f.write('Missing %:\\n')\n",
    "    f.write(miss_pct.to_string())\n",
    "print('Saved inspection.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "data.columns = [c.strip().replace(' ','_') for c in data.columns]\n",
    "\n",
    "# date -> account_age_days\n",
    "date_cols = [c for c in data.columns if any(x in c.lower() for x in ('date','created','joined'))]\n",
    "for dc in date_cols:\n",
    "    try:\n",
    "        data[dc] = pd.to_datetime(data[dc], errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "for dc in date_cols:\n",
    "    if pd.api.types.is_datetime64_any_dtype(data[dc]):\n",
    "        data['account_age_days'] = (pd.Timestamp.now() - data[dc]).dt.days\n",
    "        break\n",
    "\n",
    "if {'followers_count','friends_count'}.issubset(set(data.columns)):\n",
    "    data['follower_following_ratio'] = data['followers_count'] / data['friends_count'].replace(0, np.nan)\n",
    "\n",
    "id_cols = [c for c in data.columns if any(k in c.lower() for k in ('id','uuid','handle','account'))]\n",
    "data.drop(columns=id_cols, inplace=True, errors='ignore')\n",
    "\n",
    "high_missing = data.columns[data.isnull().mean() > 0.7].tolist()\n",
    "print('Dropping >70% missing:', high_missing)\n",
    "data.drop(columns=high_missing, inplace=True, errors='ignore')\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb68b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [c for c in data.columns if c.lower() in ('label','is_fake','fake','target','isbot','bot','class')]\n",
    "target_col = candidates[0] if candidates else None\n",
    "print('Detected target:', target_col)\n",
    "\n",
    "if target_col:\n",
    "    data[target_col].value_counts().plot(kind='bar', title='Class counts')\n",
    "    plt.savefig(os.path.join(OUTDIR,'plots/class_counts.png'))\n",
    "    plt.show()\n",
    "\n",
    "num_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for c in num_cols[:4]:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    data[c].dropna().hist(bins=40)\n",
    "    plt.title(f'Distribution: {c}')\n",
    "    plt.savefig(os.path.join(OUTDIR,f'plots/dist_{c}.png'))\n",
    "    plt.show()\n",
    "\n",
    "if len(num_cols) >= 2:\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(data[num_cols].corr(), annot=False, cmap='coolwarm')\n",
    "    plt.title('Numeric Correlation (sample)')\n",
    "    plt.savefig(os.path.join(OUTDIR,'plots/corr_heatmap.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b11646",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is None:\n",
    "    raise ValueError('Please set target_col manually in the notebook.')\n",
    "y = data[target_col].copy()\n",
    "y = y.replace({'fake':1,'real':0,'bot':1,'genuine':0,'yes':1,'no':0, True:1, False:0})\n",
    "if y.dtype == 'object':\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y.fillna('missing'))\n",
    "\n",
    "X = data.drop(columns=[target_col], errors='ignore')\n",
    "text_high = [c for c in X.select_dtypes(include=['object']).columns if X[c].nunique() > 50]\n",
    "print('Dropping high-card text columns:', text_high)\n",
    "X = X.drop(columns=text_high, errors='ignore')\n",
    "print('Final X shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01352e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y if y.nunique()>1 else None)\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b18182",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os, joblib, warnings\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import sklearn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "OUTDIR = \"./outputs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "TEST_SIZE = 0.20\n",
    "RND = 42\n",
    "\n",
    "for name in (\"X\", \"y\"):\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"{name} not found â€” prepare X (DataFrame) and y (Series) before running this cell.\")\n",
    "\n",
    "high_card_text = [c for c in X.select_dtypes(include=[\"object\"]).columns if X[c].nunique() > 50]\n",
    "if high_card_text:\n",
    "    X = X.drop(columns=high_card_text)\n",
    "    print(\"Dropped high-card text columns:\", high_card_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RND, stratify=(y if getattr(y, \"nunique\", lambda: 1)() > 1 else None)\n",
    ")\n",
    "print(\"Data shapes â€” Train:\", X_train.shape, \"Test:\", X_test.shape, \"| sklearn\", sklearn.__version__)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "major, minor = tuple(map(int, sklearn.__version__.split('.')[:2]))\n",
    "ohe_kwargs = {\"handle_unknown\":\"ignore\"}\n",
    "if (major, minor) >= (1,2):\n",
    "    ohe_kwargs[\"sparse_output\"] = False\n",
    "else:\n",
    "    ohe_kwargs[\"sparse\"] = False\n",
    "\n",
    "numeric_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "categorical_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(**ohe_kwargs))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [(\"num\", numeric_transformer, numeric_features),\n",
    "     (\"cat\", categorical_transformer, categorical_features)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(solver=\"saga\", max_iter=3000, C=0.7, penalty=\"l2\", class_weight=None, n_jobs=-1, random_state=RND),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=250, max_depth=8, min_samples_leaf=6, max_features=\"sqrt\", random_state=RND, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=200, learning_rate=0.04, max_depth=3, subsample=0.9, random_state=RND),\n",
    "}\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\",\n",
    "                                      n_estimators=250, max_depth=4, learning_rate=0.04,\n",
    "                                      subsample=0.85, colsample_bytree=0.7,\n",
    "                                      random_state=RND, n_jobs=-1, reg_alpha=1.0, reg_lambda=1.0)\n",
    "    xgb_available = True\n",
    "except Exception:\n",
    "    xgb_available = False\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "Xtr = preprocessor.transform(X_train)\n",
    "Xte = preprocessor.transform(X_test)\n",
    "\n",
    "rows = []\n",
    "trained_pipelines = {}\n",
    "for name, clf in models.items():\n",
    "    print(f\"Training {name} ...\", end=\" \")\n",
    "    try:\n",
    "        clf.fit(Xtr, y_train)\n",
    "        pipe = make_pipeline(preprocessor, clf)\n",
    "        trained_pipelines[name] = pipe\n",
    "        y_tr_pred = clf.predict(Xtr)\n",
    "        y_te_pred = clf.predict(Xte)\n",
    "        tr_acc = accuracy_score(y_train, y_tr_pred)\n",
    "        te_acc = accuracy_score(y_test, y_te_pred)\n",
    "        gap = tr_acc - te_acc\n",
    "        rows.append((name, tr_acc, te_acc, gap))\n",
    "        print(\"done.\")\n",
    "    except Exception as e:\n",
    "        print(\"FAILED:\", e)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"Model\",\"Train Acc\",\"Test Acc\",\"Gap\"]).set_index(\"Model\")\n",
    "df = df.round(4)\n",
    "print(\"\\n===== SUMMARY =====\")\n",
    "display(df)\n",
    "\n",
    "for name, pipe in trained_pipelines.items():\n",
    "    path = os.path.join(OUTDIR, f\"pipeline_{name}.joblib\")\n",
    "    try:\n",
    "        joblib.dump(pipe, path)\n",
    "    except Exception:\n",
    "        joblib.dump(pipe.steps[-1][1], os.path.join(OUTDIR, f\"estimator_{name}.joblib\"))\n",
    "print(\"Saved pipelines to\", OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa16d80",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if 'trained_pipelines' not in globals() or not trained_pipelines:\n",
    "    raise RuntimeError(\"trained_pipelines not found. Run the train cell first and ensure trained_pipelines dict exists.\")\n",
    "\n",
    "summary_rows = []\n",
    "per_model_samples = {}\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"  MODEL PERFORMANCE SUMMARY  \".center(60))\n",
    "print(\"\\n\")\n",
    "\n",
    "for name, pipe in trained_pipelines.items():\n",
    "    try:\n",
    "        estimator = pipe.steps[-1][1]\n",
    "    except Exception:\n",
    "        estimator = pipe\n",
    "\n",
    "    try:\n",
    "        preproc = pipe.steps[0][1]\n",
    "        Xte_arr = preproc.transform(X_test)\n",
    "        Xtr_arr = preproc.transform(X_train)\n",
    "    except Exception:\n",
    "        Xte_arr = X_test\n",
    "        Xtr_arr = X_train\n",
    "\n",
    "    try:\n",
    "        y_pred_test = estimator.predict(Xte_arr)\n",
    "        y_pred_train = estimator.predict(Xtr_arr)\n",
    "    except Exception:\n",
    "        y_pred_test = pipe.predict(X_test)\n",
    "        y_pred_train = pipe.predict(X_train)\n",
    "\n",
    "    y_prob_test = None\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        try:\n",
    "            y_prob_test = estimator.predict_proba(Xte_arr)[:, 1]\n",
    "        except Exception:\n",
    "            try:\n",
    "                y_prob_test = pipe.predict_proba(X_test)[:, 1]\n",
    "            except Exception:\n",
    "                y_prob_test = None\n",
    "\n",
    "    # metrics\n",
    "    train_acc = float(accuracy_score(y_train, y_pred_train))\n",
    "    test_acc  = float(accuracy_score(y_test, y_pred_test))\n",
    "    precision = float(precision_score(y_test, y_pred_test, zero_division=0))\n",
    "    recall    = float(recall_score(y_test, y_pred_test, zero_division=0))\n",
    "    f1        = float(f1_score(y_test, y_pred_test, zero_division=0))\n",
    "    gap       = train_acc - test_acc\n",
    "    cm        = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"TrainAcc\": train_acc,\n",
    "        \"TestAcc\": test_acc,\n",
    "        \"Gap\": gap,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    })\n",
    "\n",
    "    sample_indices = list(X_test.index[:5])\n",
    "    sample_list = []\n",
    "    for idx in sample_indices:\n",
    "        try:\n",
    "            pos = list(X_test.index).index(idx)\n",
    "            pred = int(y_pred_test[pos])\n",
    "            prob = float(y_prob_test[pos]) if (y_prob_test is not None and len(y_prob_test) > pos) else None\n",
    "        except Exception:\n",
    "            pred = None\n",
    "            prob = None\n",
    "        sample_list.append({\"orig_index\": idx, \"pred\": pred, \"prob\": (round(prob,4) if prob is not None else None)})\n",
    "    per_model_samples[name] = {\"confusion_matrix\": cm.tolist(), \"samples\": sample_list, \"classification_report\": classification_report(y_test, y_pred_test, zero_division=0)}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows).set_index(\"Model\")\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "print(\">> Summary table (sorted by TestAcc):\\n\")\n",
    "display(df_summary.sort_values(\"TestAcc\", ascending=False))\n",
    "\n",
    "df_summary.to_csv(os.path.join(OUTDIR, \"models_summary_metrics.csv\"))\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "for name, info in per_model_samples.items():\n",
    "    row = df_summary.loc[name]\n",
    "    print(f\"ðŸ”¹ {name} \".ljust(58, \"â”€\"))\n",
    "    print(f\"Train Acc : {row['TrainAcc']:.4f}    Test Acc : {row['TestAcc']:.4f}    Gap : {row['Gap']:.4f}\")\n",
    "    print(f\"Precision : {row['Precision']:.4f}    Recall : {row['Recall']:.4f}    F1 : {row['F1']:.4f}\")\n",
    "    print(\"\\nConfusion Matrix (rows: true, cols: pred):\")\n",
    "    cm = info[\"confusion_matrix\"]\n",
    "    cm_arr = np.array(cm)\n",
    "    cm_df = pd.DataFrame(cm_arr, columns=[f\"pred_{i}\" for i in range(cm_arr.shape[1])], index=[f\"true_{i}\" for i in range(cm_arr.shape[0])])\n",
    "    display(cm_df)\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(info[\"classification_report\"])\n",
    "    print(\"Sample predictions (first 5 test rows):\")\n",
    "    sample_df = pd.DataFrame(info[\"samples\"])\n",
    "    display(sample_df)\n",
    "    sample_df.to_csv(os.path.join(OUTDIR, f\"sample_preds_{name.replace(' ','_')}.csv\"), index=False)\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "print(\"All reports + CSVs saved to:\", OUTDIR)\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb00a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "OUTDIR = \"./outputs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "source = globals().get(\"trained_pipelines\", None) or globals().get(\"trained\", None) or globals().get(\"results\", None)\n",
    "if not source:\n",
    "    raise RuntimeError(\"No trained models found. Ensure `trained_pipelines` or `trained`/`results` exists from training step.\")\n",
    "\n",
    "roc_rows = []\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for name, entry in source.items():\n",
    "    model = None\n",
    "    pipe = None\n",
    "    if hasattr(entry, \"predict\"):\n",
    "        try:\n",
    "            if hasattr(entry, \"named_steps\"):\n",
    "                pipe = entry\n",
    "                model = entry.steps[-1][1]\n",
    "            else:\n",
    "                model = entry\n",
    "        except Exception:\n",
    "            model = entry\n",
    "    elif isinstance(entry, dict):\n",
    "        model = entry.get(\"model\") or entry.get(\"estimator\")\n",
    "        if model is None and 'trained_pipelines' in globals():\n",
    "            model = globals()['trained_pipelines'].get(name)\n",
    "            if model is not None and hasattr(model, 'steps'):\n",
    "                pipe = model\n",
    "                model = model.steps[-1][1]\n",
    "    else:\n",
    "        model = entry\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"[WARN] Could not find estimator for {name}, skipping ROC.\")\n",
    "        continue\n",
    "\n",
    "    X_te_for_model = None\n",
    "    if pipe is not None:\n",
    "        try:\n",
    "            preproc = pipe.steps[0][1]\n",
    "            X_te_for_model = preproc.transform(X_test)\n",
    "        except Exception:\n",
    "            X_te_for_model = X_test\n",
    "    else:\n",
    "        if 'preprocessor' in globals():\n",
    "            try:\n",
    "                X_te_for_model = globals()['preprocessor'].transform(X_test)\n",
    "            except Exception:\n",
    "                X_te_for_model = X_test\n",
    "        else:\n",
    "            X_te_for_model = X_test\n",
    "\n",
    "    y_score = None\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_score = model.predict_proba(X_te_for_model)[:,1]\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            y_score = model.decision_function(X_te_for_model)\n",
    "        else:\n",
    "            print(f\"[WARN] model {name} has no predict_proba/decision_function â€” skipping ROC.\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to get scores for {name}: {e} -- trying pipeline.predict_proba\")\n",
    "        try:\n",
    "            if pipe is not None and hasattr(pipe, \"predict_proba\"):\n",
    "                y_score = pipe.predict_proba(X_test)[:,1]\n",
    "        except Exception:\n",
    "            print(f\"[WARN] fallback predict_proba failed for {name}; skipping.\")\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={auc:.3f})\")\n",
    "        roc_rows.append({\"Model\": name, \"AUC\": auc})\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] ROC compute failed for {name}: {e}\")\n",
    "\n",
    "plt.plot([0,1],[0,1], linestyle='--', color='grey')\n",
    "plt.xlim([-0.02,1.02])\n",
    "plt.ylim([-0.02,1.02])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves â€” All Models\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "out_png = os.path.join(OUTDIR, \"roc_curves.png\")\n",
    "plt.savefig(out_png, bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "if roc_rows:\n",
    "    auc_df = pd.DataFrame(roc_rows).set_index(\"Model\").sort_values(\"AUC\", ascending=False)\n",
    "    auc_df.to_csv(os.path.join(OUTDIR, \"models_auc_summary.csv\"))\n",
    "    print(\"\\nSaved ROC plot ->\", out_png)\n",
    "    print(\"Saved AUC summary CSV ->\", os.path.join(OUTDIR, \"models_auc_summary.csv\"))\n",
    "    display(auc_df)\n",
    "else:\n",
    "    print(\"No ROC curves were generated (no models with probability/decision scores).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed8abc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, joblib\n",
    "import pandas as pd\n",
    "\n",
    "OUTDIR = \"./outputs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "if \"df_summary\" not in globals():\n",
    "    raise RuntimeError(\"df_summary not found. Run the summary cell first.\")\n",
    "\n",
    "if \"trained_pipelines\" not in globals() or not trained_pipelines:\n",
    "    raise RuntimeError(\"trained_pipelines dict missing. Make sure models were trained.\")\n",
    "\n",
    "\n",
    "best_row = df_summary.sort_values(\"TestAcc\", ascending=False).iloc[0]\n",
    "best_model_name = best_row.name\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"OUR DOPE MODEL\")\n",
    "print(\" \")\n",
    "print(f\"Model Name : {best_model_name}\")\n",
    "print(f\"Train Acc  : {best_row['TrainAcc']:.4f}\")\n",
    "print(f\"Test Acc   : {best_row['TestAcc']:.4f}\")\n",
    "print(f\"Gap        : {best_row['Gap']:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "best_model_pipeline = trained_pipelines[best_model_name]\n",
    "\n",
    "\n",
    "save_path = os.path.join(OUTDIR, f\"best_model_{best_model_name}.joblib\")\n",
    "\n",
    "joblib.dump(best_model_pipeline, save_path)\n",
    "\n",
    "print(f\"Our DOPE model saved successfully at:\\n   {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222bea8",
   "metadata": {},
   "source": [
    "\n",
    " Models Trained: LogisticRegression, RandomForest, GradientBoosting.\n",
    " Best model --> `outputs/best_model.joblib`.\n",
    " Key metrics --> `outputs/metrics_summary.txt`.\n",
    " Plots --> `outputs/plots/`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4eef48",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
